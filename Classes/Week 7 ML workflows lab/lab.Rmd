---
title: "Modeling species distributions with random forest"
output: html_notebook
---

## Modeling species distributions with Random Forest

In this lab, you will use data from the Alaska ground fish trawl survey conducted by the National Oceanic and Atmospheric Administration (NOAA). The survey has sampled fish and invertebrate populations in the Bering every year since 1986, providing insights into changes in the distribution and abundance of these populations over time.

The goal of this lab is to process the data for machine learning and then model the relationships between species abundance and environmental characteristics using random forest. We will utilize random forest models to identify and visualize the relationship between species and their environment.

We will also use cross-validation to estimate the model's predictive skill on new data. We will compare two approaches for dividing the data for cross-validation. One that is entirely random and another that groups the data into spatial blocks. 

This R notebook is set up to run the analysis, but I have deleted most of the code from each cell. Your task will be to fill in the notebook cells with R code, following the instructions for each step. 

Please turn in the R notebook with code sections filled out. There are several questions at the end of the notebook. Please turn in answers to these questions along with the code for the final assignment. Some code sections and questions are optional for Undergraduate students; these should be clearly marked.  

If you have questions about how the assignment will be graded, please see the rubric, which is included in the lab R project folder. 

### Step 1: Load libraries and the raw data sets 

This analysis relies on R libraries for data processing, visualization, and training random forest models. The dplyr and reshape2 libraries provide several useful functions for applying transformations and formatting data sets. lubridate helps organize information related to dates and times. ggplot2 is used to create plots, and caret provides functions for training random forest models. Finally, blockCV and sf are used for the spatial blocking procedure we will use for cross-validation. 

The analysis utilizes four datasets, two from the NOAA survey program and two from remote sensing in the Bering sea: species_data, haul_data, blooms_data, and ice_data. 

This cell loads the packages, datasets, and runs a few data processing steps. You may need to set the root directory to the folder where the lab is saved, but otherwise, it does not need modification. 

```{r}
library(dplyr)
library(reshape2)
library(ggplot2)
library(lubridate)
library(caret)
library(blockCV)
library(sf)

# Data on species presence 
species_data <- read.csv("raw_data/CATCH AND HAUL DATA .csv")

# Data on abiotic conditions at survey stations 
haul_data <- read.csv("raw_data/HAUL_DATA.csv")

#### Time series of climate conditions in the Bering Sea ###

# spring time algae blooms 
bloom_data <- read.csv("raw_data/bloom_timing.csv")
bloom_data <- bloom_data %>% mutate(Survey.year = year) %>%
  select(Survey.year, north_south, globcolour_peak_mean) %>%
  dcast(Survey.year~north_south)
names(bloom_data) <- c("Survey.year","blooms_north","blooms_south")

# sea ice cover
ice_data <- read.csv("raw_data/ice.csv") %>% 
  mutate(Survey.year = year) %>% select(-year)
```

# Select a species to analyze 

The species_data dataset contains a column named Taxon.common.name, which lists the species observed during the survey. You can use the unique function to see which species are included in the data set, and the filter function from dplyr to select one species in the data set to analyze. 

The raw data in the species_data dataset includes lots of unnecessary columns. We can simplify a bit by using the select function to restrict the data to the columns of interest `Haul.ID`, which will allow us to link the observations to the abiotic conditions observed in the haul_data data set, and `Number.CPUE..no.km2.`, which is a proxy for abundance. 

```{r}

```
# Add survey hauls that did not observe the species of interest 

Unfortunately, the species_data dataset does not include observations where the species of interest was absent, so we will need to add these back in and assign the species and abundance (`Number.CPUE..no.km2.`) of zero. The variable`Haul.ID` provides a unique identifier for each data point in the survey. You can use this variable to identify individual samples with and without the species of interest. 
 
```{r}

```

# Merge species and haul data sets 

Use the `Haul.ID` to merge the haul_data data set with covariates into the species_data dataset. You'll want to keep the columns `Bottom.temperature..degrees.Celsius.` , `Surface.temperature..degrees.Celsius.`, `Depth..m.`, `Survey.year`, `Start.latitude..decimal.degrees.`, `Start.longitude..decimal.degrees.`, `Date.and.time columns`, which give information about the time, place, depth, and water temperature of the observation. 

This is also a good place to use the lubridate library to convert the dates in the `date.and.time` column into a numeric value for the day of the year to test the effect of seasonality on the species distribution. Once you add this variable, remove the `Date.and.time` column. 

```{r}

```

# Merge the ice and blooms data into the main data set
The bloom_data and ice_data datasets do not include any spatial information, so we can merge them into the main data set using the `Survey.year` column. 

```{r}

```

# Convert abundances to presence-absence data
Create a new column that indicates observations where the species is present, and remove the column for abundances.

```{r}

```


# Scale predictors (covariates) to have a mean of zero and a variance of one.
The best practice for most supervised machine learning applications is scaling the predictors to have a mean of zero and a variance of one. This practice helps many algorithms to make apples-to-apples comparisons of the importance of each predictor. It is less critical for random forest, but it is still best practice. The function `scale` is very useful for this step.

```{r}

```

# Removing missing values
We need to handle observations that have missing values for one or more of the predictors. In this case, there are relatively few missing values, so simply removing them should be ok. 

```{r}

```

# Split into training and testing sets 
Next, we need to split the data into training and testing sets. To start, split them at random. I suggest randomly selecting 20% of the observations to remove from the training data set for model validation. Later, we will use a method for generating spatial training-validation splits. 

```{r}

```

# Format the outcome variables
The outcome variables should indicate the species presence either as a Boolean (true or false) or as a numeric outcome (0,1). However, the random forest classification algorithm will need a categorical outcome data type i.e., a factor. If your outcome data are either numeric or boolean, convert them to a factor before fitting the models. 


```{r}

```

# Train the random forest model with the caret package

The caret package (Classification And REgression Training) in R provides a useful framework for training, validating, and hyperparameter tuning many types of machine learning models. The caret library provides one primary function, `train`, which takes a model formula, data set, and model type or "method" and automatically trains and validates the model using cross-validation. 

You can control the model's validation procedure and add hyperparameter tuning by modifying the `tuneGrid` and `trControl` arguments. The `tuneGrid` argument needs a grid of hyperparameter values for the model to test. Random forest models typically work well using default hyperparamter values, but the `.mtry` parameter can effect performance. The hyperparameter `.mtry` controls the number of different variables the model considers when making a branch in each categorization tree. A smaller value of `.mtry` will mean that a smaller number of variables are tested at each split. This will increase the number of different variables represented in the model, because some splits will only consider variables with low predictive skill, increasing the chance that one of these variables is included. Check the `caret::train` documentation for details on the `tuneGrid` argument. (hint: the `expand.grid` function can help get the data in the correct format.) Test a small ~2, medium ~4, and large value ~8 for `.mtry` in the model training and tuning process.

The `trControl` argument determines the method used to compare models with different hyperparameters in the model tuning procedure. The caret package has a function `caret:tuneControl` designed to help construct the `trControl` argument in the  `train` function. Check out the documentation to see what arguments this function takes. Set up the tune control argument to run a cross-validation procedure with four folds. 

Once you have the `tuneGrid` and `trainControl` arguments set up, run the model training and tuning routine with the `caret::train` function.


```{r}

```

# Evaluate the model's skill on the validation set with ROC curves

You can write code to calculate ROC curves from scratch, but I recommend computing them using the pROC library. You can construct the receiver operator curves using the `pROC:roc` function, calculate the area under the curve with `pROC::auc`, and plot the curve using `plot`. You will need to check the documentation to see how each of these functions is implemented. 

```{r}

```

# Identify the most important predictors of species presence. 
One really nice tool for interpreting random forest models is variable importance plots. Read through the caret documentation to find a function for calculating variable importance and create a scatter plot with the results.

```{r}

```

# Visualize relationships learned by the model 

Another tool we can use to interpret the model's predictions is partial dependence plots. These show the relationship between a predictor and the outcome used by the model to make predictions. We will need a new library pdp to make these plots. Again, you can use the help function to see how this library works. Also, be careful. The pdp library does not know how your data is scaled and applies the wrong scaling by default for presence/absence data. You will need to tell the function that calculates the partial dependence plot that your model returns probabilities. 

```{r}

```
# Validate the model using spatial blocking pattern (only required for graduate students)

For species distribution models like this one, we typically want to use spatial blocks for model testing and validation. A lot of thought can go into setting up spatial blocking procedures to reflect characteristics of the data, like spatial autocorrelation and the types of new data we want the model to predict. If you are using this type of approach for a research project, I would recommend getting familiar with some of the research articles on this topic. This article (https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13107), which describes the package we are using, is a good starting point. 

Rather than becoming experts on the topic, today we will be relying on a library called blockCV, which has several methods for creating spatial blocking schemes for cross-validating species distribution models. We will use the library first to identify the extent of spatial autocorrelation in our species data using the `blockCV ::cv_spatial_autocor` and then create blocks that match this scale using the `blockCV::cv_spatial` function. 

Hint: to use blockCV we will need to reformat our data from a data.frame. Object to a spatial data frame object from using the sf library. 


1) Start by calculating the characteristic length scale. 
```{r}

# extracting data from the  blockCV (correlations) object to create a semi variogram 
distance <- correlations$variograms[[1]]$exp_var$dist
semi_var <- correlations$variograms[[1]]$exp_var$gamma
plot(distance,semi_var, xlab = "Distance (m)", ylab = "Semivar (gamma)")
```

2) Split the data into a training and a final validation set. Remove 20% of the data for final validation.
```{r}

```

3) Now that the final validation set is removed, split the data set up again into spatial blocks for cross-validation within the model tuning procedure.
```{r}

```

4) Run the model training and tuning process using `caret::train` with the spatial training - testing splits. You will have to provide the `caret::trainControl` function with the indexes for each of the training and testing folds created by the `blockCV::cv_spatial` function. This can be done by supplying the `folds_list` from the `blockCV::cv_spatial` function using the `index` and `indexOut` arguments for `caret::trainControl`. However, you will have to reformat the `folds_list` object (hint: `lapply(list, `[[`, index)` will allow you to index into the list stored within lists).   

```{r}

```

# Calculate the ROC curves on the spatial validation set

Use the same procedure as before to calculate the receiver operator characteristics. How much has the model's performance estimate changed from the random validation set?

```{r}

```


# Identify the most important predictors of species presence on the new model (only required for graduate students)

Recreate the variable importance and partial dependence plots on the model tuned using the spatial cross-validation procedure. Are the most critical variables and their relationships to species presence different than when the random splits were used? 


```{r}

```

```{r}

```
# Questions: 
1. Repeat the analysis for another species. Do these species live in similar or different habitats? Provide evidence from the variable importance and partial dependence plots.  
2. Of the two species you compared, which species would you expect to increase in abundance as climate changes increase temperatures in the Bering Sea?
3.  The data set includes observations made across many years. How about changing the cross-validation procedure to account for the temporal structure of the data set?
4.  How might you design a cross-validation routine to account for the spatial and temporal structure simultaneously?

## Grad students only 
5. How different was the model's predictive skill between the random and spatial cross-validation methods? What do the differences in accuracy estimates tell you about the structure of the data set? 
6. How sensitive is the model's performance to the hyperparameters? Did the best value of `.mtry` depend on the cross-validation method?

